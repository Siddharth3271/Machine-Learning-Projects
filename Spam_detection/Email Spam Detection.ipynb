{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b4217c4-7b6d-41d1-aa95-b8b22f05fcdc",
   "metadata": {},
   "source": [
    "# Email Spam Detection Using Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccdd52b3-e2d1-4220-8de3-cf338fb18643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Siddharth\n",
      "[nltk_data]     Shukla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "#stopwords are commonly used words in a language (like \"the\", \"and\", \"is\") that are often removed during text preprocessing because they do not carry significant meaning for tasks like classification or clustering.\n",
    "#The string module in Python is useful for various text processing tasks. It includes constants and classes that can help with common string operations such as removing punctuation, generating random strings, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc3744a",
   "metadata": {},
   "source": [
    "## Import the csv file and print first five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5759b6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  Subject: naturally irresistible your corporate...     1\n",
       "1  Subject: the stock trading gunslinger  fanny i...     1\n",
       "2  Subject: unbelievable new homes made easy  im ...     1\n",
       "3  Subject: 4 color printing special  request add...     1\n",
       "4  Subject: do not have money , get software cds ...     1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"emails.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9f32f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5728, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET THE NO. OF ROWS AND COLUMNS\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83dc26aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'spam'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CHECK THE COLUMNS LABELS\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4afb786",
   "metadata": {},
   "source": [
    "## Check for duplicates and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f3ff5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5695, 2)\n"
     ]
    }
   ],
   "source": [
    "#Removing duplicates in original dataset\n",
    "df.drop_duplicates(inplace=True)\n",
    "#Display rows and column number after removing duplicates\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b499a4ae",
   "metadata": {},
   "source": [
    "## See the no. of missing data for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38c14f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text    0\n",
      "spam    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# provide count of missing values in each column of dataframe\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b422d09c",
   "metadata": {},
   "source": [
    "## Download the Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a2e3097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Siddharth\n",
      "[nltk_data]     Shukla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fef8f087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Subject, naturally, irresistible, corporate, ...\n",
       "1    [Subject, stock, trading, gunslinger, fanny, m...\n",
       "2    [Subject, unbelievable, new, homes, made, easy...\n",
       "3    [Subject, 4, color, printing, special, request...\n",
       "4    [Subject, money, get, software, cds, software,...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now create a function to clean the text and return tokens\n",
    "#->First remove punctuation and then remove the stopwords\n",
    "def process(text):\n",
    "    # Remove punctuation\n",
    "    nopunc=[char for char in text if char not in string.punctuation]\n",
    "    nopunc=''.join(nopunc)\n",
    "    clean=[word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "    return clean\n",
    "# Now apply the function to the text data\n",
    "df['text'].head().apply(process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31fded6",
   "metadata": {},
   "source": [
    "## Convert the text into the matrix of token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f567ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "message=CountVectorizer(analyzer=process).fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42bc1519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5695, 37229)\n"
     ]
    }
   ],
   "source": [
    "#split the data 80% training and 20% test\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest,ytrain,ytest=train_test_split(message,df['spam'],test_size=0.20,random_state=0)\n",
    "# to see the shape of data\n",
    "print(message.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ece58b",
   "metadata": {},
   "source": [
    "## Create and train the Naives Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "701cbd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier=MultinomialNB().fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ff9890",
   "metadata": {},
   "source": [
    "## See the Classifiers value and actual values on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0233218f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predict(xtrain))  # predicted labels\n",
    "print(ytrain.values)     #actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08bf58c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset results:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3457\n",
      "           1       0.99      1.00      0.99      1099\n",
      "\n",
      "    accuracy                           1.00      4556\n",
      "   macro avg       0.99      1.00      1.00      4556\n",
      "weighted avg       1.00      1.00      1.00      4556\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      " [[3445   12]\n",
      " [   1 1098]]\n",
      "Accuracy: 0.9971466198419666\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on training dataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
    "pred=classifier.predict(xtrain)   # predicted labels\n",
    "print(\"Training dataset results:\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(ytrain, pred))   #actual labels\n",
    "print()\n",
    "print(\"Confusion Matrix\\n\",confusion_matrix(ytrain,pred))\n",
    "#This means:\n",
    "\n",
    "# True Positives (TP): 3445 (Actual class 0 and predicted class 0)\n",
    "# True Negatives (TN): 1098 (Actual class 1 and predicted class 1)\n",
    "# False Positives (FP): 12 (Actual class 1 but predicted as class 0)\n",
    "# False Negatives (FN): 1 (Actual class 0 but predicted as class 1)\n",
    "print(\"Accuracy:\",accuracy_score(ytrain,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11f681fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 0 0 0]\n",
      "[1 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#print the predictions on test dataset\n",
    "print(classifier.predict(xtest))\n",
    "#print the actual values on test dataset\n",
    "print(ytest.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59967f8b",
   "metadata": {},
   "source": [
    "## Evaluate the data on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5139531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset results:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       870\n",
      "           1       0.97      1.00      0.98       269\n",
      "\n",
      "    accuracy                           0.99      1139\n",
      "   macro avg       0.98      0.99      0.99      1139\n",
      "weighted avg       0.99      0.99      0.99      1139\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      " [[862   8]\n",
      " [  1 268]]\n",
      "Accuracy: 0.9920983318700615\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the test dataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
    "pred=classifier.predict(xtest)   # predicted labels\n",
    "print(\"Training dataset results:\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(ytest, pred))   #actual labels\n",
    "print()\n",
    "print(\"Confusion Matrix\\n\",confusion_matrix(ytest,pred))\n",
    "print(\"Accuracy:\",accuracy_score(ytest,pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
